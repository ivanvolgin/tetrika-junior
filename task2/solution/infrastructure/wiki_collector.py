import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from task2.solution.core.logger import get_logger
from task2.solution.core.settings import BASE_URL, START_PAGE

logger = get_logger(__name__)

def collect_category_page_links() -> list[str]:
    urls = []
    url = urljoin(BASE_URL, START_PAGE)

    while url:
        logger.info(f"–°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏: {url}")
        urls.append(url)

        resp = requests.get(url)
        soup = BeautifulSoup(resp.text, "html.parser")

        next_link = soup.find("a", string="–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
        url = urljoin(BASE_URL, next_link["href"]) if next_link else None

    logger.info(f"üîó –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(urls)}")
    return urls
